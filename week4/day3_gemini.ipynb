{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29132937-97c6-481e-a102-ecd3b31f6336",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai\n",
    "from IPython.display import Markdown, display, update_display\n",
    "import gradio as gr\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5de3673-2b3f-4a63-97cd-06615dfcb4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google API Key exists\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd03ec5e-2df9-4ea8-ac41-67d76b6ccdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6d024be-4405-42dc-8fc1-be7089599167",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that reimplements Python code in high performance C++ for an Windows system. \"\n",
    "system_message += \"Respond only with C++ code; use comments sparingly and do not provide any explanation other than occasional comments. \"\n",
    "system_message += \"The C++ response needs to produce an identical output in the fastest possible time.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2162cba8-c078-4e96-858c-622b6ad1f7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_prompt_for(python):\n",
    "    user_prompt = \"Rewrite this Python code in C++ with the fastest possible implementation that produces identical output in the least time. \"\n",
    "    user_prompt += \"Respond only with C++ code; do not explain your work other than a few comments. \"\n",
    "    user_prompt += \"Pay attention to number types to ensure no int overflows. Remember to #include all necessary C++ packages such as iomanip.\\n\\n\"\n",
    "    user_prompt += python\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf748394-4a77-487c-8751-5b82e5281f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages_for(python):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for(python)}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6dc1d31-1f2f-48ad-baee-84b90763149b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_output(cpp):\n",
    "    code = cpp.replace(\"```cpp\", \"\").replace(\"```\", \"\")\n",
    "    with open(\"optimized.cpp\", \"w\") as f:\n",
    "        f.write(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e58381f0-b84f-4fec-b79c-2b4f8dadfb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_google(python):\n",
    "    reply = \"\"\n",
    "\n",
    "    gemini = google.generativeai.GenerativeModel(\n",
    "        model_name=\"gemini-1.5-pro\",\n",
    "        system_instruction=system_message\n",
    "    )\n",
    "\n",
    "    response = gemini.generate_content(\n",
    "        user_prompt_for(python),\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    for chunk in response:\n",
    "        if chunk.text:\n",
    "            reply += chunk.text\n",
    "            print(chunk.text, end=\"\", flush=True)\n",
    "\n",
    "    write_output(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a081bd92-1cf6-4b5a-bce8-19d1b27a48db",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = \"\"\"\n",
    "import time\n",
    "\n",
    "def calculate(iterations, param1, param2):\n",
    "    result = 1.0\n",
    "    for i in range(1, iterations+1):\n",
    "        j = i * param1 - param2\n",
    "        result -= (1/j)\n",
    "        j = i * param1 + param2\n",
    "        result += (1/j)\n",
    "    return result\n",
    "\n",
    "start_time = time.time()\n",
    "result = calculate(100_000_000, 4, 1) * 4\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Result: {result:.12f}\")\n",
    "print(f\"Execution Time: {(end_time - start_time):.6f} seconds\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d993a968-6847-4afd-b852-d7fca46a8ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 3.141592658589\n",
      "Execution Time: 13.346197 seconds\n"
     ]
    }
   ],
   "source": [
    "exec(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c75cefe-c5d2-4da6-9d0e-d9db484bd88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```cpp\n",
      "#include <iostream>\n",
      "#include <iomanip>\n",
      " <chrono>\n",
      "\n",
      "int main() {\n",
      "00000000;long iterations = 1\n",
      "    int param1 = 4;\n",
      "    int param2 = 1;\n",
      "\n",
      "time = std::chrono::high_resolution_clock::now();\n",
      "\n",
      "    double result = 1.0;\n",
      " 1; i <= iterations; ++i) {\n",
      "        long long j = i * param1 - param2;\n",
      "        result -= (1.0 / j);\n",
      "        j = i * param1 + param2;\n",
      "        result += (1.0 / j);\n",
      "    }\n",
      "    result *= 4.0;\n",
      "\n",
      "    auto end_time = std::chrono::high_resolution_clock::now();\n",
      "duration<double> elapsed_time = end_time - start_time;\n",
      "\n",
      "\n",
      "    std::cout << std::fixed << std::setprecision(12) << \"Result: \" << result << std::endl;\n",
      " \"Execution Time: \" << elapsed_time.count() << \" seconds\" << std::endl;\n",
      "\n",
      "    return 0;\n",
      "}\n",
      "\n",
      "```"
     ]
    }
   ],
   "source": [
    "optimize_google(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f567eea5-cd25-4897-876f-29ad9f3f2928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_google(python):\n",
    "    # Initialize empty reply string\n",
    "    reply = \"\"\n",
    "    \n",
    "    # The API for Gemini has a slightly different structure\n",
    "    gemini = google.generativeai.GenerativeModel(\n",
    "        model_name='gemini-1.5-pro',\n",
    "        system_instruction=system_message\n",
    "    )\n",
    "    \n",
    "    response = gemini.generate_content(\n",
    "        user_prompt_for(python),\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    # Process the stream\n",
    "    for chunk in response:\n",
    "        # Extract text from the chunk\n",
    "        if chunk.text:\n",
    "            reply += chunk.text\n",
    "            yield reply.replace('```cpp\\n','').replace('```','')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86b78502-5625-4b89-b732-0846314f3abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(python, model):\n",
    "    result=stream_google(python)\n",
    "    for stream_so_far in result:\n",
    "        yield stream_so_far    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b61a603-8012-45b5-9cc1-aff8d283dda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gr.Blocks() as ui:\n",
    "    with gr.Row():\n",
    "        python = gr.Textbox(label=\"Python code:\", lines=10, value=pi)\n",
    "        cpp = gr.Textbox(label=\"C++ code:\", lines=10)\n",
    "    with gr.Row():\n",
    "        model = gr.Dropdown([\"Google\"], label=\"Select model\", value=\"Google\")\n",
    "        convert = gr.Button(\"Convert code\")\n",
    "\n",
    "    convert.click(optimize, inputs=[python, model], outputs=[cpp])\n",
    "\n",
    "ui.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cada3b44-2b00-4bcd-aed7-fe49ee8ac571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_python(code):\n",
    "    try:\n",
    "        output = io.StringIO()\n",
    "        sys.stdout = output\n",
    "        exec(code)\n",
    "    finally:\n",
    "        sys.stdout = sys.__stdout__\n",
    "    return output.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c65f1d16-fbe3-4327-8acb-0acb8e94d0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_cpp(code):\n",
    "    write_output(code)\n",
    "    try:\n",
    "        compile_cmd = [\"g++\", \"-o\", \"optimized\", \"optimized.cpp\"]\n",
    "        compile_result = subprocess.run(compile_cmd, check=True, text=True, capture_output=True)\n",
    "        run_cmd = [\"./optimized\"]\n",
    "        run_result = subprocess.run(run_cmd, check=True, text=True, capture_output=True)\n",
    "        return run_result.stdout\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        return f\"An error occurred:\\n{e.stderr}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db3efb63-4925-42bf-a91e-1ae6872df774",
   "metadata": {},
   "outputs": [],
   "source": [
    "css = \"\"\"\n",
    ".python {background-color: #306998;}\n",
    ".cpp {background-color: #050;}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66a82c8a-7c9a-403e-8255-ff7d0d49c33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\seoli\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\seoli\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\seoli\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 2047, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\seoli\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 1594, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\seoli\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\seoli\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2505, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\seoli\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 1005, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\seoli\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\utils.py\", line 869, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\seoli\\AppData\\Local\\Temp\\ipykernel_616\\1027701432.py\", line 5, in execute_cpp\n",
      "    compile_result = subprocess.run(compile_cmd, check=True, text=True, capture_output=True)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\seoli\\anaconda3\\envs\\llms\\Lib\\subprocess.py\", line 548, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\seoli\\anaconda3\\envs\\llms\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"C:\\Users\\seoli\\anaconda3\\envs\\llms\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [WinError 2] 지정된 파일을 찾을 수 없습니다\n"
     ]
    }
   ],
   "source": [
    "with gr.Blocks(css=css) as ui:\n",
    "    gr.Markdown(\"## Convert code from Python to C++\")\n",
    "    with gr.Row():\n",
    "        python = gr.Textbox(label=\"Python code:\", value=pi, lines=10)\n",
    "        cpp = gr.Textbox(label=\"C++ code:\", lines=10)\n",
    "    with gr.Row():\n",
    "        model = gr.Dropdown([\"Google\"], label=\"Select model\", value=\"Google\")\n",
    "    with gr.Row():\n",
    "        convert = gr.Button(\"Convert code\")\n",
    "    with gr.Row():\n",
    "        python_run = gr.Button(\"Run Python\")\n",
    "        cpp_run = gr.Button(\"Run C++\")\n",
    "    with gr.Row():\n",
    "        python_out = gr.TextArea(label=\"Python result:\", elem_classes=[\"python\"])\n",
    "        cpp_out = gr.Textbox(label=\"C++ result:\", elem_classes=[\"cpp\"])\n",
    "\n",
    "    convert.click(optimize, inputs=[python, model], outputs=[cpp])\n",
    "    python_run.click(execute_python, inputs=[python], outputs=[python_out])\n",
    "    cpp_run.click(execute_cpp, inputs=[cpp], outputs=[cpp_out])\n",
    "\n",
    "ui.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0931734-56d7-401a-b6ca-54c95a2f061b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
