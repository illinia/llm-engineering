import os
import json
from dotenv import load_dotenv
from IPython.display import Markdown, display, update_display
from openai import OpenAI
import gradio as gr
import google.generativeai
import anthropic


# constants

MODEL_GPT = 'gpt-4o-mini'
MODEL_CLAUDE = 'claude-3-5-sonnet-20240620'
MODEL_GEMINI = 'gemini-1.5-flash'


# set up environment

load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')
os.environ['ANTHROPIC_API_KEY'] = os.getenv('ANTHROPIC_API_KEY', 'your-key-if-not-using-env')
os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY', 'your-key-if-not-using-env')


openai = OpenAI()
claude = anthropic.Anthropic()
google.generativeai.configure()


import base64
from io import BytesIO
from PIL import Image
from IPython.display import Audio, display


def talker(message):
    response = openai.audio.speech.create(
        model='tts-1',
        voice='alloy',
        input=message
    )

    audio_stream = BytesIO(response.content)
    output_filename = 'output_audio.mp3'
    with open(output_filename, 'wb') as f:
        f.write(audio_stream.read())

    display(Audio(output_filename,autoplay=True))

talker('테스트')


# prompts
general_prompt = "답변은 가능한 한 기술적으로 하세요.\
전문 지식이 있는 주제에 대한 질문에만 답변하세요.\
모르는 것이 있으면 말하세요. 한글로만 답변해줘."

additional_prompt_gpt = "사용자 질의를 분석하고 콘텐츠가 주로 \
거시경제 뉴스 분석이 있는지 확인합니다. \
그렇다면 직접 답변하세요. 그렇지 않으면 주로 \
자산 간 상관관계 분석 관련이 있는 경우 도구 ask_gemini에서 답변을 받거나 \
기술적 지표 분석과 관련된 주제에 속하는 경우 도구 ask_claude에서 답변을 받습니다."

system_prompt_gpt = "You are a helpful technical tutor who is an expert in \
거시경제 뉴스 분석."+ additional_prompt_gpt + general_prompt
system_prompt_gemini = "You are a helpful technical tutor who is an expert in 자산 간 상관관계 분석." + general_prompt
system_prompt_claude = "You are a helpful technical tutor who is an expert in 기술적 지표 분석." + general_prompt

def get_user_prompt(question):
    return "Please give a detailed explanation to the following question: " + question


def call_claude(question):
    result = claude.messages.create(
        model=MODEL_CLAUDE,
        max_tokens=200,
        temperature=0.3,
        system=system_prompt_claude,
        messages=[
            {"role": "user", "content": get_user_prompt(question)},
        ],
    )

    return result.content[0].text


def call_gemini(question):
    gemini = google.generativeai.GenerativeModel(
        model_name=MODEL_GEMINI,
        system_instruction=system_prompt_gemini
    )
    response = gemini.generative_content(get_user_prompt(question))
    response = response.text
    return response


def ask_claude(question):
    print(f"Tool ask_claude called for {question}")
    return call_claude(question)

def ask_gemini(question):
    print(f"Tool ask_gemini called for {question}")
    return call_gemini(question)


ask_claude_function = {
    "name": "ask_claude",
    "description": "Get the answer to the question related to a topic this agent is faimiliar with.",
    "parameters": {
        "type": "object",
        "properties": {
            "question_for_topic": {
                "type": "string",
                "description": "질문은 기술적 지표 분석과 관련이 있다."
            }
        },
        "required": ["question_for_topic"],
        "additionalProperties": False
    }
}


ask_gemini_function = {
    "name": "ask_gemini",
    "description": "Get the answer to the question related to a topic this agent is faimiliar with.",
    "parameters": {
        "type": "object",
        "properties": {
            "question_for_topic": {
                "type": "string",
                "description": "질문은 자산 간 상관관계 분석과 관련이 있다."
            }
        },
        "required": ["question_for_topic"],
        "additionalProperties": False
    }
}


tools = [{"type": "function", "function": ask_claude_function},
        {"type": "function", "function": ask_gemini_function}]
tools_function_map = {
    "ask_claude": ask_claude,
    "ask_gemini": ask_gemini
}


def chat(history):
    messages = [{"role": "system", "content": system_prompt_gpt}] + history
    stream = openai.chat.completions.create(model=MODEL_GPT, messages=messages, tools=tools, stream=True)

    full_response = ""
    history += [{"role": "assistant", "content": full_response}]

    tool_call_accumulator = ""
    tool_call_id = None
    tool_call_function_name = None
    tool_calls = []

    for chunk in stream:
        if chunk.choices[0].delta.content:
            full_response += chunk.choices[0].delta.content or ""
            history[-1]['content'] = full_response
            yield history

        if chunk.choices.delta.tool_calls:
            message = chunk.choices[0].delta
            print("hunk.choices[0].delta " + message.tool_calls)
            for tc in chunk.choices[0].delta.tool_calls:
                if tc.id:
                    too_call_id = tc.id
                    if tool_call_function_name is None:
                        tool_call_function_name = tc.function.name

                tool_call_accumulator += tc.function.arguments if tc.function.arguments else ""

                try:
                    func_args = json.loads(tool_call_accumulator)

                    


def handle_tool_call(function_name, arguments, tool_call_id):
    question = arguments.get('question_for_topic')

    tool_call = {
        "id": tool_call_id,
        "type": "function",
        "function": {
            "name": function_name,
            "arguments": json.dumps(arguments)
        }
    }

    if function_name in tools_function_map:
        answer = tools_function_map[function_name](question)
        response = {
            "role": "tool",
            "content": json.dumps({"question": question, "answer": answer}),
            "tool_call_id": tool_call_id
        }
        return response, tool_call



